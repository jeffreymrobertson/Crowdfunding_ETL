# ETL Mini Project: Building an Efficient Crowdfunding Data Pipeline

Welcome to the ETL Mini Project! In this collaborative endeavor, we will be working with Python, Pandas, and Postgres to build an Extract, Transform, and Load (ETL) pipeline for crowdfunding data. Our primary goal is to extract data from Excel files, perform necessary transformations, create CSV files, and import the data into a Postgres database.

## Project Overview
The ETL process is a fundamental step in data engineering and analysis, allowing us to extract valuable information, transform it into a structured format, and load it into a database for further analysis. Throughout this one-week project, we will apply ETL principles to crowdfunding data and create four essential DataFrames: the "category" DataFrame, the "subcategory" DataFrame, the "campaign" DataFrame, and the "contacts" DataFrame.



## Project Structure
1. Create the Category and Subcategory DataFrames.
2. Create the Campaign DataFrame
3. Create the Contacts DataFrame
4. Create the Crowdfunding Database


## Work Allocation
### Create the Category and Subcategory DataFrames
- 

### Create the Contacts DataFrame
- 

### Create the Campaign DataFrame
- Jeffrey Robertson
1. Rename the column names
2. Convert the columns goal and pledged frrom objects to floaaats
3. Check to ensure that  the data types were changed
4. Convert the launch date and end date into date time format
5. Merge with the category and subcategory dataframes and drop unneeded columns from the dataframe
6. Save to csv file

### Create the Crowdfunding Database
- Riddhi Mistry

### Technology Used
The ETL Mini Project will utilize the following technologies:
- **Python**: We will use Python programming language for data extraction, transformation, and various data manipulation tasks.
- **Pandas**: Pandas will be used for data processing and to create DataFrames for the crowdfunding data.
- **Postgres**: Postgres will serve as our relational database management system to store and manage the extracted and transformed data.
- **SQLAlchemy**: SQLAlchemy will be used to interact with the Postgres database and perform data insertion and querying.
- **Jupyter Notebook**: We will work with Jupyter Notebook to execute and document our data processing steps.



